{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsWmOJRlTZeax2n39qoyCJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mx60AHwYH-3"
      },
      "outputs": [],
      "source": [
        "!pip install neo4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase\n",
        "from neo4j.exceptions import ServiceUnavailable, AuthError\n",
        "import pandas as pd\n",
        "\n",
        "# Use the credentials that were successful in the previous step\n",
        "uri = \"blank\"\n",
        "user = \"neo4j\"\n",
        "password = \"blank\""
      ],
      "metadata": {
        "id": "kUdfM7XFYQYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e41133a"
      },
      "source": [
        "## Query Database Schema\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66a1c60f"
      },
      "source": [
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "try:\n",
        "    # Create a session to execute queries\n",
        "    with driver.session() as session:\n",
        "        # Retrieve all node labels\n",
        "        result_labels = session.run(\"CALL db.labels()\")\n",
        "        node_labels = [record[0] for record in result_labels]\n",
        "\n",
        "        # Retrieve all relationship types\n",
        "        result_rels = session.run(\"CALL db.relationshipTypes()\")\n",
        "        relationship_types = [record[0] for record in result_rels]\n",
        "\n",
        "        # Print the retrieved information\n",
        "        print(\"Node Labels:\", node_labels)\n",
        "        print(\"Relationship Types:\", relationship_types)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # Close the driver\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "877bf4e8"
      },
      "source": [
        "## Rank Diseases by Degree\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f08eccef"
      },
      "source": [
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "query = \"\"\"\n",
        "MATCH (d:Disease)-[r]-()\n",
        "RETURN d.name AS Disease, count(r) AS Degree\n",
        "ORDER BY Degree DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query)\n",
        "        data = [record.data() for record in result]\n",
        "        df_degree = pd.DataFrame(data)\n",
        "\n",
        "    # Display the resulting DataFrame\n",
        "    print(df_degree)\n",
        "\n",
        "finally:\n",
        "    # Close the driver\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3f102bd"
      },
      "source": [
        "## Visualize Relationship Counts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cdb7104"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "query = \"\"\"\n",
        "MATCH ()-[r]->()\n",
        "RETURN type(r) AS RelationshipType, count(r) AS Count\n",
        "ORDER BY Count DESC\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query)\n",
        "        data = [record.data() for record in result]\n",
        "        df_rels = pd.DataFrame(data)\n",
        "\n",
        "    # Display the DataFrame head for verification\n",
        "    print(df_rels.head())\n",
        "\n",
        "    # Visualize the distribution of relationship types\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(data=df_rels, x='Count', y='RelationshipType', hue='RelationshipType', palette='viridis', legend=False)\n",
        "    plt.title('Distribution of Relationship Types')\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel('Relationship Type')\n",
        "    plt.show()\n",
        "\n",
        "finally:\n",
        "    # Close the driver\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "195af12e"
      },
      "source": [
        "## Calculate Advanced Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bde2881"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse.linalg import eigs\n",
        "\n",
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        # Calculate Degree Extremes\n",
        "        # Using size() or count{} pattern for efficiency if available, or aggregating.\n",
        "        # Note: count{(n)--()} is efficient for calculating degree.\n",
        "        query_degree = \"\"\"\n",
        "        MATCH (n)\n",
        "        RETURN max(count{(n)--()}) AS MaxDegree, min(count{(n)--()}) AS MinDegree\n",
        "        \"\"\"\n",
        "        result_degree = session.run(query_degree).single()\n",
        "        max_degree = result_degree[\"MaxDegree\"]\n",
        "        min_degree = result_degree[\"MinDegree\"]\n",
        "\n",
        "        # Fetch Graph Structure for Spectral Radius\n",
        "        # Fetch all node IDs to create a mapping\n",
        "        # Using elementId() instead of deprecated id()\n",
        "        query_nodes = \"MATCH (n) RETURN elementId(n) AS id\"\n",
        "        result_nodes = session.run(query_nodes)\n",
        "        node_ids = [record[\"id\"] for record in result_nodes]\n",
        "\n",
        "        # Map Neo4j internal IDs to sequential indices 0..N-1\n",
        "        id_to_index = {node_id: i for i, node_id in enumerate(node_ids)}\n",
        "        num_nodes = len(node_ids)\n",
        "\n",
        "        # Fetch all relationships (source, target)\n",
        "        query_rels = \"MATCH (s)-[]->(t) RETURN elementId(s) AS source, elementId(t) AS target\"\n",
        "        result_rels = session.run(query_rels)\n",
        "\n",
        "        sources = []\n",
        "        targets = []\n",
        "        for record in result_rels:\n",
        "            # Only include relationships where both nodes are in our node list (snapshot consistency)\n",
        "            if record[\"source\"] in id_to_index and record[\"target\"] in id_to_index:\n",
        "                sources.append(id_to_index[record[\"source\"]])\n",
        "                targets.append(id_to_index[record[\"target\"]])\n",
        "\n",
        "        # Build Adjacency Matrix\n",
        "        # Create data array of ones\n",
        "        data = np.ones(len(sources))\n",
        "\n",
        "        # Construct sparse matrix (N x N)\n",
        "        adj_matrix = coo_matrix((data, (sources, targets)), shape=(num_nodes, num_nodes), dtype=float)\n",
        "\n",
        "        # Calculate Spectral Radius\n",
        "        # Compute the largest magnitude eigenvalue\n",
        "        # k=1 for one eigenvalue, which='LM' for largest magnitude\n",
        "        evals, evecs = eigs(adj_matrix, k=1, which='LM')\n",
        "        spectral_radius = np.abs(evals[0])\n",
        "\n",
        "        # Display Results\n",
        "        print(f\"Max Degree: {max_degree}\")\n",
        "        print(f\"Min Degree: {min_degree}\")\n",
        "        print(f\"Spectral Radius: {spectral_radius:.4f}\")\n",
        "\n",
        "finally:\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0952844"
      },
      "source": [
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        # Count total nodes\n",
        "        result_nodes = session.run(\"MATCH (n) RETURN count(n) AS TotalNodes\")\n",
        "        total_nodes = result_nodes.single()[\"TotalNodes\"]\n",
        "\n",
        "        # Count total relationships\n",
        "        result_rels = session.run(\"MATCH ()-[r]->() RETURN count(r) AS TotalRelationships\")\n",
        "        total_rels = result_rels.single()[\"TotalRelationships\"]\n",
        "\n",
        "        # Create a summary DataFrame\n",
        "        df_invariants = pd.DataFrame([{\n",
        "            \"Metric\": \"Total Nodes\",\n",
        "            \"Count\": total_nodes\n",
        "        }, {\n",
        "            \"Metric\": \"Total Relationships\",\n",
        "            \"Count\": total_rels\n",
        "        }])\n",
        "\n",
        "    # Display the summary table\n",
        "    print(df_invariants)\n",
        "\n",
        "finally:\n",
        "    # Close the driver\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca706331"
      },
      "source": [
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "query = \"\"\"\n",
        "MATCH (c:Compound)-[:TREATS]->(d)\n",
        "WITH c, d, rand() AS r\n",
        "ORDER BY r\n",
        "LIMIT 1\n",
        "RETURN c.name AS Compound, d.name AS Disease\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query).single()\n",
        "        if result:\n",
        "            print(f\"Random Compound: {result['Compound']}\")\n",
        "            print(f\"Treats: {result['Disease']}\")\n",
        "        else:\n",
        "            print(\"No compound found with TREATS relationship.\")\n",
        "\n",
        "finally:\n",
        "    # Close the driver\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b5d8b07"
      },
      "source": [
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "query = \"\"\"\n",
        "MATCH (c:Compound {name: 'Topiramate'})-[:TREATS]->(d:Disease)\n",
        "RETURN d.name AS Disease\n",
        "ORDER BY Disease\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query)\n",
        "        diseases = [record[\"Disease\"] for record in result]\n",
        "\n",
        "    print(f\"Topiramate treats {len(diseases)} diseases:\")\n",
        "    for disease in diseases:\n",
        "        print(f\"- {disease}\")\n",
        "\n",
        "finally:\n",
        "    # Close the driver\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "Wg94cDbZ9G4P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fb87793"
      },
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a9eb532"
      },
      "source": [
        "!pip install pykeen torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "639d5d56"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "Fetch a dataset of relationships from Neo4j, separating specific Topiramate 'TREATS' relationships into a held-out test set and using the rest for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20debb35"
      },
      "source": [
        "import pandas as pd\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        # Fetch Test Set (Topiramate TREATS Disease)\n",
        "        print(\"Fetching Test Set...\")\n",
        "        query_test = \"\"\"\n",
        "        MATCH (s:Compound {name: 'Topiramate'})-[r:TREATS]->(t:Disease)\n",
        "        RETURN s.name AS source, type(r) AS relation, t.name AS target\n",
        "        \"\"\"\n",
        "        result_test = session.run(query_test)\n",
        "        df_test = pd.DataFrame([r.data() for r in result_test])\n",
        "\n",
        "        # Fetch Background Training Set (50k random relations)\n",
        "        print(\"Fetching Background Training Set...\")\n",
        "        query_bg = \"\"\"\n",
        "        MATCH (s)-[r]->(t)\n",
        "        WHERE s.name IS NOT NULL AND t.name IS NOT NULL\n",
        "        RETURN s.name AS source, type(r) AS relation, t.name AS target\n",
        "        LIMIT 50000\n",
        "        \"\"\"\n",
        "        result_bg = session.run(query_bg)\n",
        "        df_bg = pd.DataFrame([r.data() for r in result_bg])\n",
        "\n",
        "        # Fetch Topiramate Context (All relations involving Topiramate)\n",
        "        print(\"Fetching Topiramate Context...\")\n",
        "        query_ctx = \"\"\"\n",
        "        MATCH (s)-[r]->(t)\n",
        "        WHERE s.name = 'Topiramate' OR t.name = 'Topiramate'\n",
        "        RETURN s.name AS source, type(r) AS relation, t.name AS target\n",
        "        \"\"\"\n",
        "        result_ctx = session.run(query_ctx)\n",
        "        df_ctx = pd.DataFrame([r.data() for r in result_ctx])\n",
        "\n",
        "    # Create Final Train Set\n",
        "    # Concatenate background and context\n",
        "    df_train_raw = pd.concat([df_bg, df_ctx]).drop_duplicates()\n",
        "\n",
        "    # Filter out Test leakage from Train Set\n",
        "    if not df_test.empty:\n",
        "        # Perform an anti-join to remove rows in df_train_raw that match df_test\n",
        "        df_train = df_train_raw.merge(df_test, on=['source', 'relation', 'target'], how='left', indicator=True)\n",
        "        df_train = df_train[df_train['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
        "    else:\n",
        "        df_train = df_train_raw\n",
        "\n",
        "    # Reset index for cleanliness\n",
        "    df_train = df_train.reset_index(drop=True)\n",
        "    df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nTest Set (Held-out):\")\n",
        "    print(df_test)\n",
        "    print(f\"\\nTraining Set Shape: {df_train.shape}\")\n",
        "    print(f\"Test Set Shape: {df_test.shape}\")\n",
        "\n",
        "finally:\n",
        "    # Close the driver\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bb76992"
      },
      "source": [
        "\n",
        "Convert the training DataFrame into a PyKEEN TriplesFactory and train a TransE model using the pipeline function for 5 epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a529c42"
      },
      "source": [
        "import pandas as pd\n",
        "from neo4j import GraphDatabase\n",
        "from pykeen.pipeline import pipeline\n",
        "from pykeen.triples import TriplesFactory\n",
        "import torch\n",
        "\n",
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        # Fetch Context for Test Targets (Diseases)\n",
        "        # We need to ensure the diseases in the test set exist in the training graph.\n",
        "        test_diseases = df_test['target'].unique().tolist()\n",
        "        print(f\"Fetching context for test diseases: {test_diseases}\")\n",
        "\n",
        "        # Fetch relationships involving these diseases, EXCLUDING the specific Topiramate TREATS connections\n",
        "        query_disease_ctx = \"\"\"\n",
        "        MATCH (s)-[r]->(t)\n",
        "        WHERE (s.name IN $diseases OR t.name IN $diseases)\n",
        "        AND NOT (s.name = 'Topiramate' AND t.name IN $diseases AND type(r)='TREATS')\n",
        "        RETURN s.name AS source, type(r) AS relation, t.name AS target\n",
        "        LIMIT 2000\n",
        "        \"\"\"\n",
        "        result = session.run(query_disease_ctx, diseases=test_diseases)\n",
        "        df_disease_ctx = pd.DataFrame([r.data() for r in result])\n",
        "\n",
        "    print(f\"Fetched {len(df_disease_ctx)} context triples for diseases.\")\n",
        "\n",
        "    # Augment Training Set\n",
        "    # Combine the existing df_train with the new disease context\n",
        "    df_train_augmented = pd.concat([df_train, df_disease_ctx]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    # Ensure No Leakage\n",
        "    # Remove any triples that match the test set exactly\n",
        "    if not df_test.empty:\n",
        "        df_train_final = df_train_augmented.merge(df_test, on=['source', 'relation', 'target'], how='left', indicator=True)\n",
        "        df_train_final = df_train_final[df_train_final['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
        "    else:\n",
        "        df_train_final = df_train_augmented\n",
        "\n",
        "    print(f\"Final Training Set Size: {len(df_train_final)}\")\n",
        "\n",
        "\n",
        "finally:\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8612fe0"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import warnings\n",
        "import logging\n",
        "from pykeen.pipeline import pipeline\n",
        "from pykeen.triples import TriplesFactory\n",
        "\n",
        "# Suppress warnings and logs to avoid stderr output being flagged as error\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger('pykeen').setLevel(logging.ERROR)\n",
        "logging.getLogger('torch_max_mem').setLevel(logging.ERROR)\n",
        "\n",
        "# Ensure data is available (loaded from previous steps)\n",
        "if 'df_train_final' in locals() and 'df_test' in locals():\n",
        "    # Prepare PyKEEN Factories\n",
        "    training_triples = df_train_final[['source', 'relation', 'target']].astype(str).values\n",
        "    testing_triples = df_test[['source', 'relation', 'target']].astype(str).values\n",
        "\n",
        "    training_factory = TriplesFactory.from_labeled_triples(training_triples)\n",
        "\n",
        "    # Create testing factory using the mappings from the training factory\n",
        "    testing_factory = TriplesFactory.from_labeled_triples(\n",
        "        testing_triples,\n",
        "        entity_to_id=training_factory.entity_to_id,\n",
        "        relation_to_id=training_factory.relation_to_id,\n",
        "    )\n",
        "\n",
        "    print(f\"Testing factory prepared with {testing_factory.num_triples} triples.\")\n",
        "\n",
        "    # Train TransE Model\n",
        "    print(\"Starting training...\")\n",
        "    # Using explicit batch sizes\n",
        "    result = pipeline(\n",
        "        training=training_factory,\n",
        "        testing=testing_factory,\n",
        "        model='TransE',\n",
        "        epochs=5,\n",
        "        random_seed=42,\n",
        "        device='cpu',\n",
        "        training_kwargs={'batch_size': 256},\n",
        "        evaluation_kwargs={'batch_size': 64}\n",
        "    )\n",
        "\n",
        "    print(\"Training finished successfully.\")\n",
        "\n",
        "    # Display Training Loss\n",
        "    if result.losses:\n",
        "        print(f\"Final Training Loss: {result.losses[-1]:.4f}\")\n",
        "\n",
        "    # Display Evaluation Metrics\n",
        "    try:\n",
        "        mrr = result.get_metric('mrr')\n",
        "        hits10 = result.get_metric('hits@10')\n",
        "        print(f\"Test MRR: {mrr:.4f}\")\n",
        "        print(f\"Test Hits@10: {hits10:.4f}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Could not retrieve specific metric: {e}\")\n",
        "\n",
        "    # 6. Predict Scores for Held-out Topiramate Relationships\n",
        "    print(\"\\nPrediction Scores for Held-out Topiramate Triples (Higher is better):\")\n",
        "    model = result.model\n",
        "    model.to('cpu')\n",
        "\n",
        "    # Get the mapped triples (indices) for the test set\n",
        "    mapped_test_triples = testing_factory.mapped_triples\n",
        "\n",
        "    # Calculate scores using the model\n",
        "    with torch.no_grad():\n",
        "        scores = model.score_hrt(mapped_test_triples)\n",
        "\n",
        "    # Display the score for each triple\n",
        "    for i, (src, rel, tgt) in enumerate(testing_triples):\n",
        "        print(f\"{src} --[{rel}]--> {tgt} : Score = {scores[i].item():.4f}\")\n",
        "\n",
        "    # Visualize training loss\n",
        "    result.plot_losses()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Error: Required DataFrames 'df_train_final' and 'df_test' are missing.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Walks"
      ],
      "metadata": {
        "id": "IquHo9ahYise"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a36a82cb"
      },
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "# Check and install neo4j if missing\n",
        "try:\n",
        "    from neo4j import GraphDatabase\n",
        "except ImportError:\n",
        "    print(\"neo4j module not found. Installing...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"neo4j\"])\n",
        "    from neo4j import GraphDatabase\n",
        "\n",
        "# Helper to ensure driver is available\n",
        "if 'driver' not in locals() or driver is None:\n",
        "    if 'uri' in locals() and 'user' in locals() and 'password' in locals():\n",
        "            driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "    else:\n",
        "            # Try to recover credentials from variables if setup cells ran\n",
        "            try:\n",
        "                driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "            except NameError:\n",
        "                print(\"Connection details (uri, user, password) not found. Please run setup cells.\")\n",
        "                driver = None\n",
        "\n",
        "# Re-build matrix if missing\n",
        "if 'adj_matrix' not in locals() or 'node_ids' not in locals():\n",
        "    print(\"Dependencies missing. Re-building adjacency matrix from Neo4j...\")\n",
        "    if driver:\n",
        "        try:\n",
        "            with driver.session() as session:\n",
        "                # Fetch Nodes\n",
        "                print(\"  Fetching nodes...\")\n",
        "                result_nodes = session.run(\"MATCH (n) RETURN elementId(n) AS id\")\n",
        "                node_ids = [record[\"id\"] for record in result_nodes]\n",
        "                id_to_index = {node_id: i for i, node_id in enumerate(node_ids)}\n",
        "                num_nodes = len(node_ids)\n",
        "\n",
        "                # Fetch Relationships\n",
        "                print(\"  Fetching relationships...\")\n",
        "                result_rels = session.run(\"MATCH (s)-[]->(t) RETURN elementId(s) AS source, elementId(t) AS target\")\n",
        "                sources = []\n",
        "                targets = []\n",
        "                for record in result_rels:\n",
        "                    if record[\"source\"] in id_to_index and record[\"target\"] in id_to_index:\n",
        "                        sources.append(id_to_index[record[\"source\"]])\n",
        "                        targets.append(id_to_index[record[\"target\"]])\n",
        "\n",
        "                # Build Matrix\n",
        "                print(\"  Constructing sparse matrix...\")\n",
        "                data = np.ones(len(sources))\n",
        "                adj_matrix = coo_matrix((data, (sources, targets)), shape=(num_nodes, num_nodes), dtype=float)\n",
        "                print(\"  Matrix reconstruction complete.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error rebuilding matrix: {e}\")\n",
        "    else:\n",
        "        print(\"Cannot build matrix: Driver not initialized.\")\n",
        "\n",
        "# Fetch Node Names for Display (New Section)\n",
        "if 'node_names_map' not in locals() and driver:\n",
        "    print(\"Fetching node names for better readability...\")\n",
        "    try:\n",
        "        with driver.session() as session:\n",
        "            # Fetch 'name' property, fallback to 'id' property (common in biological graphs)\n",
        "            # If neither exists, it will be None (handled in display loop)\n",
        "            query = \"MATCH (n) RETURN elementId(n) AS id, COALESCE(n.name, n.id) AS label\"\n",
        "            result = session.run(query)\n",
        "            node_names_map = {record[\"id\"]: record[\"label\"] for record in result}\n",
        "            print(f\"  Mapped names for {len(node_names_map)} nodes.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Warning: Could not fetch node names: {e}\")\n",
        "        node_names_map = {}\n",
        "\n",
        "# Proceed with Random Walk\n",
        "if 'adj_matrix' in locals() and 'node_ids' in locals():\n",
        "    print(\"Preparing graph for random walk...\")\n",
        "\n",
        "    # Tweak: Make graph undirected to ignore directionality\n",
        "    print(\"  Symmetrizing adjacency matrix (treating graph as undirected)...\")\n",
        "    undirected_adj = adj_matrix + adj_matrix.T\n",
        "\n",
        "    # Convert to CSR\n",
        "    csr_adj = undirected_adj.tocsr()\n",
        "\n",
        "    def get_random_walk(start_node_idx, walk_length=10):\n",
        "        \"\"\"Performs a random walk starting from a given node index.\"\"\"\n",
        "        walk = [start_node_idx]\n",
        "        current_node = start_node_idx\n",
        "\n",
        "        for _ in range(walk_length - 1):\n",
        "            start_ptr = csr_adj.indptr[current_node]\n",
        "            end_ptr = csr_adj.indptr[current_node + 1]\n",
        "            neighbors = csr_adj.indices[start_ptr:end_ptr]\n",
        "\n",
        "            if len(neighbors) == 0:\n",
        "                break # Dead end\n",
        "\n",
        "            next_node = np.random.choice(neighbors)\n",
        "            walk.append(next_node)\n",
        "            current_node = next_node\n",
        "\n",
        "        return walk\n",
        "\n",
        "    # Parameters for the simulation\n",
        "    num_samples = 10\n",
        "    length_of_walk = 5\n",
        "\n",
        "    # Select random starting nodes\n",
        "    if len(node_ids) > 0:\n",
        "        count = min(num_samples, len(node_ids))\n",
        "        start_nodes_indices = np.random.choice(len(node_ids), count, replace=False)\n",
        "\n",
        "        print(f\"\\nPerforming {count} Monte Carlo random walks (length {length_of_walk})...\\n\")\n",
        "\n",
        "        for start_idx in start_nodes_indices:\n",
        "            # Generate walk\n",
        "            path_indices = get_random_walk(start_idx, length_of_walk)\n",
        "\n",
        "            # Map indices back to Element IDs\n",
        "            path_ids = [node_ids[i] for i in path_indices]\n",
        "\n",
        "            # Map Element IDs to Names (if available)\n",
        "            path_names = []\n",
        "            for pid in path_ids:\n",
        "                name = node_names_map.get(pid) if 'node_names_map' in locals() else None\n",
        "                path_names.append(name if name else pid)\n",
        "\n",
        "            print(f\"Walk starting at {path_names[0]}:\\n -> Path: {path_names}\")\n",
        "            print(\"-\" * 40)\n",
        "    else:\n",
        "        print(\"No nodes found in the graph.\")\n",
        "\n",
        "else:\n",
        "    print(\"Critical Error: Failed to initialize graph data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8d19b10"
      },
      "source": [
        "# Analyze Graph Connectivity to explain short walks\n",
        "if 'csr_adj' in locals():\n",
        "    # In a CSR matrix, the difference between indptr values gives the number of non-zero elements per row (out-degree)\n",
        "    out_degrees = np.diff(csr_adj.indptr)\n",
        "\n",
        "    # Count nodes with 0 outgoing edges\n",
        "    dead_ends = np.sum(out_degrees == 0)\n",
        "    total_nodes = len(out_degrees)\n",
        "    percentage_dead_ends = (dead_ends / total_nodes) * 100\n",
        "\n",
        "    print(f\"Analysis of Graph Connectivity:\")\n",
        "    print(f\"  Total Nodes: {total_nodes}\")\n",
        "    print(f\"  Nodes with 0 outgoing edges (Dead Ends): {dead_ends}\")\n",
        "    print(f\"  Percentage of Dead Ends: {percentage_dead_ends:.2f}%\")\n",
        "\n",
        "    if percentage_dead_ends > 0:\n",
        "        print(\"\\nConclusion: The random walks are shorter than expected because the walker frequently lands on these 'dead end' nodes, where it has no neighbors to move to next.\")\n",
        "else:\n",
        "    print(\"Adjacency matrix (csr_adj) not found. Please run the random walk cell first.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "953ea777"
      },
      "source": [
        "# Initialize Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        # Fetch Element IDs for Start (Obesity) and Target (Topiramate)\n",
        "        # We use these to find the integer index in our matrix\n",
        "        result = session.run(\"\"\"\n",
        "        MATCH (t:Compound {name: 'Topiramate'}), (s:Disease {name: 'obesity'})\n",
        "        RETURN elementId(t) AS target_id, elementId(s) AS start_id\n",
        "        \"\"\").single()\n",
        "\n",
        "        if result:\n",
        "            target_eid = result[\"target_id\"]\n",
        "            start_eid = result[\"start_id\"]\n",
        "\n",
        "            # Map Element IDs to Matrix Indices\n",
        "            # Ensure node_ids and id_to_index are available (from previous cells)\n",
        "            if 'node_ids' in locals():\n",
        "                id_to_index = {node_id: i for i, node_id in enumerate(node_ids)}\n",
        "\n",
        "                if target_eid in id_to_index and start_eid in id_to_index:\n",
        "                    target_index = id_to_index[target_eid]\n",
        "                    start_index = id_to_index[start_eid]\n",
        "                    print(f\"Start Node: 'obesity' (Index {start_index})\")\n",
        "                    print(f\"Target Node: 'Topiramate' (Index {target_index})\")\n",
        "                else:\n",
        "                    print(\"Error: Start or Target node not found in the loaded graph indices.\")\n",
        "            else:\n",
        "                print(\"Error: 'node_ids' list is missing. Please run the Random Walk initialization cell first.\")\n",
        "        else:\n",
        "            print(\"Error: Could not find 'Topiramate' or 'obesity' in the database.\")\n",
        "finally:\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a99876bd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Check dependencies\n",
        "if 'adj_matrix' in locals() and 'start_index' in locals() and 'target_index' in locals() and 'perform_mcts' in globals():\n",
        "    # Ensure Undirected Graph (Symmetrize)\n",
        "    # The user suspects directionality is an issue. Let's force symmetrization.\n",
        "    print(\"Ensuring adjacency matrix is symmetric (undirected)...\")\n",
        "    sym_adj = adj_matrix + adj_matrix.T\n",
        "    csr_adj = sym_adj.tocsr() # Update the global csr_adj used by MCTS\n",
        "\n",
        "    print(f\"Original Start: {start_index} (Obesity)\")\n",
        "    print(f\"Target: {target_index} (Topiramate)\")\n",
        "\n",
        "    # Get neighbors of obesity (distance 1) using the undirected matrix\n",
        "    s_ptr = csr_adj.indptr[start_index]\n",
        "    e_ptr = csr_adj.indptr[start_index + 1]\n",
        "    neighbors_d1 = csr_adj.indices[s_ptr:e_ptr]\n",
        "\n",
        "    # Get neighbors of neighbors (distance 2)\n",
        "    candidates_d2 = set()\n",
        "    for n1 in neighbors_d1:\n",
        "        s_ptr_2 = csr_adj.indptr[n1]\n",
        "        e_ptr_2 = csr_adj.indptr[n1 + 1]\n",
        "        neighbors_d2 = csr_adj.indices[s_ptr_2:e_ptr_2]\n",
        "        candidates_d2.update(neighbors_d2)\n",
        "\n",
        "    # Filter\n",
        "    # Remove obesity itself\n",
        "    if start_index in candidates_d2:\n",
        "        candidates_d2.remove(start_index)\n",
        "    # Remove Topiramate (if present at d=2, though it's also at d=1)\n",
        "    if target_index in candidates_d2:\n",
        "        candidates_d2.remove(target_index)\n",
        "\n",
        "    candidates_list = list(candidates_d2)\n",
        "\n",
        "    if candidates_list:\n",
        "        # Select random new start node\n",
        "        new_start_index = np.random.choice(candidates_list)\n",
        "\n",
        "        # Get Display Name\n",
        "        new_start_eid = node_ids[new_start_index]\n",
        "        new_start_name = str(new_start_eid)\n",
        "        if 'node_names_map' in locals():\n",
        "            new_start_name = node_names_map.get(new_start_eid, new_start_name)\n",
        "\n",
        "        print(f\"\\nSelected New Start Node (2 hops away): '{new_start_name}' (Index {new_start_index})\")\n",
        "\n",
        "        # Execute MCTS\n",
        "        # Using 200,000 iterations for deep search\n",
        "        print(\"-\" * 30)\n",
        "        path_indices = perform_mcts(new_start_index, target_index, iterations=200000)\n",
        "\n",
        "        if path_indices:\n",
        "            print(f\"\\nPath Found (Length {len(path_indices)}):\")\n",
        "            # Construct readable path\n",
        "            readable_path = []\n",
        "            for idx in path_indices:\n",
        "                eid = node_ids[idx]\n",
        "                name = node_names_map.get(eid, str(eid)) if 'node_names_map' in locals() else str(eid)\n",
        "                readable_path.append(name)\n",
        "\n",
        "            print(\" -> \".join(readable_path))\n",
        "        else:\n",
        "            print(\"\\nTarget not found within iteration limit.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No valid candidates found at distance 2.\")\n",
        "else:\n",
        "    print(\"Error: Required data (adj_matrix, indices, or MCTS function) is missing.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb086d98"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Ensure csr_adj is accessible\n",
        "if 'csr_adj' not in globals():\n",
        "    print(\"Error: 'csr_adj' not found in global scope. Please run the Random Walk initialization cell.\")\n",
        "else:\n",
        "    class MCTSNode:\n",
        "        def __init__(self, state_index, parent=None):\n",
        "            self.state_index = state_index\n",
        "            self.parent = parent\n",
        "            self.children = []\n",
        "            self.visits = 0\n",
        "            self.value = 0.0\n",
        "\n",
        "            # Populate neighbors using the CSR matrix for efficiency\n",
        "            # Access global csr_adj directly\n",
        "            start_ptr = csr_adj.indptr[state_index]\n",
        "            end_ptr = csr_adj.indptr[state_index + 1]\n",
        "            self.untried_neighbors = csr_adj.indices[start_ptr:end_ptr].tolist()\n",
        "\n",
        "        def is_fully_expanded(self):\n",
        "            return len(self.untried_neighbors) == 0\n",
        "\n",
        "        def best_child(self, c_param=1.41):\n",
        "            # UCB1 Selection\n",
        "            choices_weights = [\n",
        "                (child.value / child.visits) + c_param * math.sqrt((2 * math.log(self.visits) / child.visits))\n",
        "                for child in self.children\n",
        "            ]\n",
        "            return self.children[choices_weights.index(max(choices_weights))]\n",
        "\n",
        "        def expand(self):\n",
        "            # Pop a neighbor and create a child node\n",
        "            new_state_index = self.untried_neighbors.pop()\n",
        "            child_node = MCTSNode(new_state_index, parent=self)\n",
        "            self.children.append(child_node)\n",
        "            return child_node\n",
        "\n",
        "        def roll_out(self, target_index, max_steps=50):\n",
        "            # Simulation: Random Walk\n",
        "            current_state = self.state_index\n",
        "            for _ in range(max_steps):\n",
        "                if current_state == target_index:\n",
        "                    return 1.0 # Reward found\n",
        "\n",
        "                # Get neighbors\n",
        "                start_ptr = csr_adj.indptr[current_state]\n",
        "                end_ptr = csr_adj.indptr[current_state + 1]\n",
        "                neighbors = csr_adj.indices[start_ptr:end_ptr]\n",
        "\n",
        "                if len(neighbors) == 0:\n",
        "                    break\n",
        "\n",
        "                current_state = np.random.choice(neighbors)\n",
        "\n",
        "            return 0.0\n",
        "\n",
        "        def backpropagate(self, result):\n",
        "            self.visits += 1\n",
        "            self.value += result\n",
        "            if self.parent:\n",
        "                self.parent.backpropagate(result)\n",
        "\n",
        "    def perform_mcts(start_idx, target_idx, iterations=10000):\n",
        "        # 0. Quick Check: Are they direct neighbors?\n",
        "        start_ptr = csr_adj.indptr[start_idx]\n",
        "        end_ptr = csr_adj.indptr[start_idx + 1]\n",
        "        neighbors = csr_adj.indices[start_ptr:end_ptr]\n",
        "\n",
        "        print(f\"DEBUG: Start Node Degree: {len(neighbors)}\")\n",
        "        if target_idx in neighbors:\n",
        "            print(\"DEBUG: Target is a direct neighbor! Path found immediately.\")\n",
        "            return [start_idx, target_idx]\n",
        "\n",
        "        root = MCTSNode(start_idx)\n",
        "\n",
        "        print(f\"Starting MCTS search for {iterations} iterations...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        found_path = None\n",
        "\n",
        "        for i in range(iterations):\n",
        "            node = root\n",
        "\n",
        "            # Selection\n",
        "            while node.is_fully_expanded() and node.children:\n",
        "                node = node.best_child()\n",
        "                if node.state_index == target_idx:\n",
        "                    break\n",
        "\n",
        "            # Expansion\n",
        "            if not node.is_fully_expanded() and node.state_index != target_idx:\n",
        "                node = node.expand()\n",
        "\n",
        "            # Check success\n",
        "            if node.state_index == target_idx:\n",
        "                path = []\n",
        "                curr = node\n",
        "                while curr:\n",
        "                    path.append(curr.state_index)\n",
        "                    curr = curr.parent\n",
        "                found_path = path[::-1]\n",
        "                print(f\"Target found at iteration {i}!\")\n",
        "                break\n",
        "\n",
        "            # Simulation\n",
        "            reward = node.roll_out(target_idx, max_steps=50)\n",
        "\n",
        "            # Backpropagation\n",
        "            node.backpropagate(reward)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        print(f\"Search completed in {duration:.4f} seconds.\")\n",
        "\n",
        "        return found_path\n",
        "\n",
        "    # Execute Search\n",
        "    if 'start_index' in locals() and 'target_index' in locals():\n",
        "        path_indices = perform_mcts(start_index, target_index, iterations=10000)\n",
        "\n",
        "        if path_indices:\n",
        "            print(f\"Path Found (Length {len(path_indices)}):\")\n",
        "\n",
        "            # Construct readable path\n",
        "            readable_path = []\n",
        "            for idx in path_indices:\n",
        "                eid = node_ids[idx]\n",
        "                name = node_names_map.get(eid, str(eid)) if 'node_names_map' in locals() else str(eid)\n",
        "                readable_path.append(name)\n",
        "\n",
        "            print(\" -> \".join(readable_path))\n",
        "        else:\n",
        "            print(\"Target not found within iteration limit.\")\n",
        "    else:\n",
        "        print(\"Start or Target index not defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
