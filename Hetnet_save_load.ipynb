{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1xqe61efAWo-QxJE3oi1hBLaRytiSQnmW",
      "authorship_tag": "ABX9TyMWc3o0Cws91VAPIFQoWjsw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mx60AHwYH-3"
      },
      "outputs": [],
      "source": [
        "!pip install neo4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase\n",
        "from neo4j.exceptions import ServiceUnavailable, AuthError\n",
        "import pandas as pd\n",
        "\n",
        "# Use the credentials that were successful in the previous step\n",
        "uri = \"blank\"\n",
        "user = \"neo4j\"\n",
        "password = \"blank\""
      ],
      "metadata": {
        "id": "kUdfM7XFYQYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb8fea3a"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Ensure connection is established using variables from the previous cells\n",
        "try:\n",
        "    if 'driver' not in locals() or driver is None:\n",
        "        driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "    driver.verify_connectivity()\n",
        "    print(\"Connection established.\")\n",
        "except NameError:\n",
        "    print(\"Error: Please ensure 'uri', 'user', and 'password' are defined and run in the previous cells.\")\n",
        "except Exception as e:\n",
        "    print(f\"Connection failed: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52cc566b"
      },
      "source": [
        "def save_graph_to_csv(driver, output_folder):\n",
        "    \"\"\"Exports all nodes and relationships from Neo4j to CSV files.\"\"\"\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "        print(f\"Created directory: {output_folder}\")\n",
        "\n",
        "    with driver.session() as session:\n",
        "        # 1. Export Nodes by Label\n",
        "        print(\"Fetching node labels...\")\n",
        "        labels_result = session.run(\"CALL db.labels()\")\n",
        "        labels = [r[\"label\"] for r in labels_result]\n",
        "\n",
        "        for label in labels:\n",
        "            print(f\"Exporting nodes with label: {label}...\")\n",
        "            # Retrieve properties and the system ID (using elementId for Neo4j 5+)\n",
        "            query = f\"MATCH (n:`{label}`) RETURN elementId(n) as _id, properties(n) as props\"\n",
        "            try:\n",
        "                result = session.run(query)\n",
        "                data = []\n",
        "                for record in result:\n",
        "                    row = record[\"props\"]\n",
        "                    row[\"_id\"] = record[\"_id\"]\n",
        "                    data.append(row)\n",
        "\n",
        "                if data:\n",
        "                    df = pd.DataFrame(data)\n",
        "                    filename = os.path.join(output_folder, f\"nodes_{label}.csv\")\n",
        "                    df.to_csv(filename, index=False)\n",
        "                    print(f\"  Saved {len(df)} nodes to {filename}\")\n",
        "                else:\n",
        "                    print(f\"  No nodes found for label {label}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error exporting label {label}: {e}\")\n",
        "\n",
        "        # 2. Export Relationships by Type\n",
        "        print(\"Fetching relationship types...\")\n",
        "        types_result = session.run(\"CALL db.relationshipTypes()\")\n",
        "        rel_types = [r[\"relationshipType\"] for r in types_result]\n",
        "\n",
        "        for rel_type in rel_types:\n",
        "            print(f\"Exporting relationships of type: {rel_type}...\")\n",
        "            query = f\"\"\"\n",
        "            MATCH (a)-[r:`{rel_type}`]->(b)\n",
        "            RETURN elementId(startNode(r)) as _start_id, elementId(endNode(r)) as _end_id, properties(r) as props\n",
        "            \"\"\"\n",
        "            try:\n",
        "                result = session.run(query)\n",
        "                data = []\n",
        "                for record in result:\n",
        "                    row = record[\"props\"]\n",
        "                    row[\"_start_id\"] = record[\"_start_id\"]\n",
        "                    row[\"_end_id\"] = record[\"_end_id\"]\n",
        "                    data.append(row)\n",
        "\n",
        "                if data:\n",
        "                    df = pd.DataFrame(data)\n",
        "                    filename = os.path.join(output_folder, f\"rels_{rel_type}.csv\")\n",
        "                    df.to_csv(filename, index=False)\n",
        "                    print(f\"  Saved {len(df)} relationships to {filename}\")\n",
        "                else:\n",
        "                    print(f\"  No relationships found for type {rel_type}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error exporting relationship {rel_type}: {e}\")\n",
        "\n",
        "# Define the output directory\n",
        "output_directory = \"neo4j_export\" # @param {type:\"string\"}\n",
        "\n",
        "# Run the export\n",
        "if 'driver' in locals():\n",
        "    save_graph_to_csv(driver, output_directory)\n",
        "    print(f\"\\nExport completed. Files are saved in '{output_directory}'\")\n",
        "else:\n",
        "    print(\"Driver not defined. Cannot export.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71691499"
      },
      "source": [
        "def clear_database(driver, batch_size=200):\n",
        "    \"\"\"Clears the database in batches to avoid transaction timeouts.\"\"\"\n",
        "    print(f\"Clearing database with batch size {batch_size}...\")\n",
        "    query = f\"\"\"\n",
        "    MATCH (n)\n",
        "    WITH n LIMIT {batch_size}\n",
        "    DETACH DELETE n\n",
        "    RETURN count(n) as deleted_count\n",
        "    \"\"\"\n",
        "\n",
        "    total_deleted = 0\n",
        "    with driver.session() as session:\n",
        "        while True:\n",
        "            try:\n",
        "                result = session.run(query)\n",
        "                count = result.single()[\"deleted_count\"]\n",
        "                total_deleted += count\n",
        "                print(f\"  Deleted {count} nodes/relationships...\")\n",
        "                if count == 0:\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f\"Error clearing database: {e}\")\n",
        "                raise e\n",
        "    print(f\"Database cleared. Total deleted nodes: {total_deleted}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e57ede74"
      },
      "source": [
        "import math\n",
        "\n",
        "def import_from_csv(driver, input_folder, batch_size=1000):\n",
        "    \"\"\"Imports nodes and relationships from CSV files in the folder.\"\"\"\n",
        "    if not os.path.exists(input_folder):\n",
        "        print(f\"Input folder '{input_folder}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    # Helper to clean NaN values from dictionary (Neo4j doesn't like NaNs)\n",
        "    def clean_row(row):\n",
        "        return {k: v for k, v in row.items() if pd.notna(v)}\n",
        "\n",
        "    with driver.session() as session:\n",
        "        # 1. Create Constraint/Index for fast lookup during relationship creation\n",
        "        print(\"Creating temporary index for import...\")\n",
        "        # Using a generic label '_MigrationNode' for all imported nodes to allow global lookup\n",
        "        try:\n",
        "            # Try creating a constraint (preferred for uniqueness) or index\n",
        "            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (n:_MigrationNode) REQUIRE n.migration_id IS UNIQUE\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Note: Could not create constraint (might be not supported on this edition), trying index. Error: {e}\")\n",
        "            session.run(\"CREATE INDEX IF NOT EXISTS FOR (n:_MigrationNode) ON (n.migration_id)\")\n",
        "\n",
        "        # 2. Import Nodes\n",
        "        files = [f for f in os.listdir(input_folder) if f.startswith(\"nodes_\") and f.endswith(\".csv\")]\n",
        "        for file in files:\n",
        "            label = file.replace(\"nodes_\", \"\").replace(\".csv\", \"\")\n",
        "            print(f\"Importing nodes for label: {label}...\")\n",
        "            # Set low_memory=False to avoid DtypeWarning\n",
        "            df = pd.read_csv(os.path.join(input_folder, file), low_memory=False)\n",
        "\n",
        "            # Batched Import\n",
        "            total_rows = len(df)\n",
        "            chunks = math.ceil(total_rows / batch_size)\n",
        "\n",
        "            for i in range(chunks):\n",
        "                chunk = df.iloc[i*batch_size : (i+1)*batch_size]\n",
        "                batch_data = []\n",
        "                for _, row in chunk.iterrows():\n",
        "                    row_dict = row.to_dict()\n",
        "                    migration_id = row_dict.pop(\"_id\") # Extract migration ID\n",
        "                    props = clean_row(row_dict)\n",
        "                    batch_data.append({\"migration_id\": migration_id, \"props\": props})\n",
        "\n",
        "                query = f\"\"\"\n",
        "                UNWIND $batch AS row\n",
        "                CREATE (n:`{label}`:_MigrationNode)\n",
        "                SET n += row.props, n.migration_id = row.migration_id\n",
        "                \"\"\"\n",
        "                session.run(query, batch=batch_data)\n",
        "            print(f\"  Imported {total_rows} nodes.\")\n",
        "\n",
        "        # 3. Import Relationships\n",
        "        files = [f for f in os.listdir(input_folder) if f.startswith(\"rels_\") and f.endswith(\".csv\")]\n",
        "        for file in files:\n",
        "            rel_type = file.replace(\"rels_\", \"\").replace(\".csv\", \"\")\n",
        "            print(f\"Importing relationships of type: {rel_type}...\")\n",
        "            df = pd.read_csv(os.path.join(input_folder, file), low_memory=False)\n",
        "\n",
        "            # Batched Import\n",
        "            total_rows = len(df)\n",
        "            chunks = math.ceil(total_rows / batch_size)\n",
        "\n",
        "            for i in range(chunks):\n",
        "                chunk = df.iloc[i*batch_size : (i+1)*batch_size]\n",
        "                batch_data = []\n",
        "                for _, row in chunk.iterrows():\n",
        "                    row_dict = row.to_dict()\n",
        "                    start_id = row_dict.pop(\"_start_id\")\n",
        "                    end_id = row_dict.pop(\"_end_id\")\n",
        "                    props = clean_row(row_dict)\n",
        "                    batch_data.append({\"start_id\": start_id, \"end_id\": end_id, \"props\": props})\n",
        "\n",
        "                # Match by migration_id and create relationship\n",
        "                # Note: Curly braces are escaped as {{...}} for Python f-string\n",
        "                query = f\"\"\"\n",
        "                UNWIND $batch AS row\n",
        "                MATCH (start:_MigrationNode {{migration_id: row.start_id}})\n",
        "                MATCH (end:_MigrationNode {{migration_id: row.end_id}})\n",
        "                CREATE (start)-[r:`{rel_type}`]->(end)\n",
        "                SET r += row.props\n",
        "                \"\"\"\n",
        "                session.run(query, batch=batch_data)\n",
        "            print(f\"  Imported {total_rows} relationships.\")\n",
        "\n",
        "        # 4. Cleanup\n",
        "        print(\"Cleaning up temporary migration properties...\")\n",
        "        session.run(\"MATCH (n:_MigrationNode) REMOVE n:_MigrationNode, n.migration_id\")\n",
        "\n",
        "        print(\"Dropping temporary indices/constraints...\")\n",
        "        try:\n",
        "            # Drop Constraints by name\n",
        "            result = session.run(\"SHOW CONSTRAINTS YIELD name, labelsOrTypes, properties\")\n",
        "            for record in result:\n",
        "                if record[\"labelsOrTypes\"] and \"_MigrationNode\" in record[\"labelsOrTypes\"] and \\\n",
        "                   record[\"properties\"] and \"migration_id\" in record[\"properties\"]:\n",
        "                    print(f\"  Dropping constraint: {record['name']}\")\n",
        "                    session.run(f\"DROP CONSTRAINT {record['name']}\")\n",
        "\n",
        "            # Drop Indexes by name\n",
        "            result = session.run(\"SHOW INDEXES YIELD name, labelsOrTypes, properties\")\n",
        "            for record in result:\n",
        "                if record[\"labelsOrTypes\"] and \"_MigrationNode\" in record[\"labelsOrTypes\"] and \\\n",
        "                   record[\"properties\"] and \"migration_id\" in record[\"properties\"]:\n",
        "                    print(f\"  Dropping index: {record['name']}\")\n",
        "                    session.run(f\"DROP INDEX {record['name']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Cleanup of index/constraint failed: {e}\")\n",
        "\n",
        "        print(\"Import complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dee9dc2a"
      },
      "source": [
        "# Run the process\n",
        "if 'driver' in locals():\n",
        "    try:\n",
        "        print(\"Restarting database reload with final cleanup fix...\")\n",
        "        # Use smaller batch sizes to prevent MemoryPoolOutOfMemoryError\n",
        "        clear_database(driver, batch_size=200)\n",
        "        import_from_csv(driver, \"neo4j_export\", batch_size=500)\n",
        "    except Exception as e:\n",
        "        print(f\"Process stopped due to error: {e}\")\n",
        "else:\n",
        "    print(\"Driver not defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0952844"
      },
      "source": [
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        # Count total nodes\n",
        "        result_nodes = session.run(\"MATCH (n) RETURN count(n) AS TotalNodes\")\n",
        "        total_nodes = result_nodes.single()[\"TotalNodes\"]\n",
        "\n",
        "        # Count total relationships\n",
        "        result_rels = session.run(\"MATCH ()-[r]->() RETURN count(r) AS TotalRelationships\")\n",
        "        total_rels = result_rels.single()[\"TotalRelationships\"]\n",
        "\n",
        "        # Create a summary DataFrame\n",
        "        df_invariants = pd.DataFrame([{\n",
        "            \"Metric\": \"Total Nodes\",\n",
        "            \"Count\": total_nodes\n",
        "        }, {\n",
        "            \"Metric\": \"Total Relationships\",\n",
        "            \"Count\": total_rels\n",
        "        }])\n",
        "\n",
        "    # Display the summary table\n",
        "    print(df_invariants)\n",
        "\n",
        "finally:\n",
        "    # Close the driver\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bde2881"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse.linalg import eigs\n",
        "\n",
        "# Initialize the Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        # 1. Calculate Degree Extremes\n",
        "        # Using size() or count{} pattern for efficiency if available, or aggregating.\n",
        "        # Note: count{(n)--()} is efficient for calculating degree.\n",
        "        query_degree = \"\"\"\n",
        "        MATCH (n)\n",
        "        RETURN max(count{(n)--()}) AS MaxDegree, min(count{(n)--()}) AS MinDegree\n",
        "        \"\"\"\n",
        "        result_degree = session.run(query_degree).single()\n",
        "        max_degree = result_degree[\"MaxDegree\"]\n",
        "        min_degree = result_degree[\"MinDegree\"]\n",
        "\n",
        "        # 2. Fetch Graph Structure for Spectral Radius\n",
        "        # Fetch all node IDs to create a mapping\n",
        "        # Using elementId() instead of deprecated id()\n",
        "        query_nodes = \"MATCH (n) RETURN elementId(n) AS id\"\n",
        "        result_nodes = session.run(query_nodes)\n",
        "        node_ids = [record[\"id\"] for record in result_nodes]\n",
        "\n",
        "        # Map Neo4j internal IDs to sequential indices 0..N-1\n",
        "        id_to_index = {node_id: i for i, node_id in enumerate(node_ids)}\n",
        "        num_nodes = len(node_ids)\n",
        "\n",
        "        # Fetch all relationships (source, target)\n",
        "        query_rels = \"MATCH (s)-[]->(t) RETURN elementId(s) AS source, elementId(t) AS target\"\n",
        "        result_rels = session.run(query_rels)\n",
        "\n",
        "        sources = []\n",
        "        targets = []\n",
        "        for record in result_rels:\n",
        "            # Only include relationships where both nodes are in our node list (snapshot consistency)\n",
        "            if record[\"source\"] in id_to_index and record[\"target\"] in id_to_index:\n",
        "                sources.append(id_to_index[record[\"source\"]])\n",
        "                targets.append(id_to_index[record[\"target\"]])\n",
        "\n",
        "        # 3. Build Adjacency Matrix\n",
        "        # Create data array of ones\n",
        "        data = np.ones(len(sources))\n",
        "\n",
        "        # Construct sparse matrix (N x N)\n",
        "        adj_matrix = coo_matrix((data, (sources, targets)), shape=(num_nodes, num_nodes), dtype=float)\n",
        "\n",
        "        # 4. Calculate Spectral Radius\n",
        "        # Compute the largest magnitude eigenvalue\n",
        "        # k=1 for one eigenvalue, which='LM' for largest magnitude\n",
        "        evals, evecs = eigs(adj_matrix, k=1, which='LM')\n",
        "        spectral_radius = np.abs(evals[0])\n",
        "\n",
        "        # 5. Display Results\n",
        "        print(f\"Max Degree: {max_degree}\")\n",
        "        print(f\"Min Degree: {min_degree}\")\n",
        "        print(f\"Spectral Radius: {spectral_radius:.4f}\")\n",
        "\n",
        "finally:\n",
        "    driver.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
